{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "from notebooks.MultiRegression import test_dataset\n",
    "\n",
    "use_columns = [\"game_no\", \"point_no\", \"set_no\", \"p1_score\", \"p2_score\", \"p1_games\", \"p2_games\", \"server\"]\n",
    "\n",
    "rate = 0.5\n",
    "\n",
    "data = pd.read_csv(\"../data/data.csv\")\n",
    "\n",
    "x = data[use_columns]\n",
    "x[[\"elo\", \"match_prob\"]]= pd.read_csv(\"../data/elo_data.csv\")[[\"sf_elo_diff_538\", \"match_prob_kls_EM\"]]\n",
    "\n",
    "x[\"server\"] = x[\"server\"].replace(2, 0)\n",
    "y = data[\"point_victor\"]\n",
    "\n",
    "y = pd.DataFrame(y, columns=[\"point_victor\"])\n",
    "y = y.replace(2, 0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "# x_train = x[:-200]\n",
    "# x_test = x[-200:]\n",
    "# y_train = y[:-200]\n",
    "# y_test = y[-200:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d775d384cb30327"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, precision_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def test_dataset(x_train, x_test, y_train, y_test):\n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_test_backup = x_test\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    print(\"\\nRandom Forest Regression:\")\n",
    "    rf = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\n",
    "    rf.fit(x_train, y_train.ravel())\n",
    "    pred2_rf = rf.predict(x_test)\n",
    "    mse_rf = mean_squared_error(y_test.ravel(), pred2_rf)\n",
    "    mae_rf = mean_absolute_error(y_test.ravel(), pred2_rf)\n",
    "    r2_rf = r2_score(y_test.ravel(), pred2_rf)\n",
    "    print('mse: %.4f' % mse_rf)\n",
    "    print('mae: %.4f' % mae_rf)\n",
    "    print('r2: %.4f' % r2_rf)\n",
    "\n",
    "    print(\"\\nDecision Tree Regression:\")\n",
    "    dt = DecisionTreeRegressor(max_depth=10)\n",
    "    dt.fit(x_train, y_train.ravel())\n",
    "    pred2_dt = dt.predict(x_test)\n",
    "    mse_dt = mean_squared_error(y_test.ravel(), pred2_dt)\n",
    "    mae_dt = mean_absolute_error(y_test.ravel(), pred2_dt)\n",
    "    r2_dt = r2_score(y_test.ravel(), pred2_dt)\n",
    "    print('mse: %.4f' % mse_dt)\n",
    "    print('mae: %.4f' % mae_dt)\n",
    "    print('r2: %.4f' % r2_dt)\n",
    "\n",
    "    print(\"Linear\")\n",
    "    lir = LinearRegression()\n",
    "    lir.fit(x_train, y_train.ravel())\n",
    "    pred2_lir = lir.predict(x_test)\n",
    "\n",
    "    plt.plot(pred2_lir[0:50], label='Linear Regression')\n",
    "\n",
    "    mse_lir = mean_squared_error(y_test.ravel(), pred2_lir)\n",
    "    mae_lir = mean_absolute_error(y_test.ravel(), pred2_lir)\n",
    "    r2_lir = r2_score(y_test.ravel(), pred2_lir)\n",
    "    print('mse: %.4f' % mse_lir)\n",
    "    print('mae: %.4f' % mae_lir)\n",
    "    print('r2: %.4f' % r2_lir)\n",
    "\n",
    "    print(\"\\nGradient Boosting Regression:\")\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    gbr.fit(x_train, y_train.ravel())\n",
    "    pred2_gbr = gbr.predict(x_test)\n",
    "    mse_gbr = mean_squared_error(y_test.ravel(), pred2_gbr)\n",
    "    mae_gbr = mean_absolute_error(y_test.ravel(), pred2_gbr)\n",
    "    r2_gbr = r2_score(y_test.ravel(), pred2_gbr)\n",
    "    print('mse: %.4f' % mse_gbr)\n",
    "    print('mae: %.4f' % mae_gbr)\n",
    "    print('r2: %.4f' % r2_gbr)\n",
    "\n",
    "    print(\"\\nLGBMRegression:\")\n",
    "    gmb = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=20, force_col_wise=True, verbose=-1)\n",
    "    gmb.fit(x_train, y_train.ravel())\n",
    "    pred2_gmb = gmb.predict(x_test)\n",
    "\n",
    "    plt.plot(pred2_gmb[0:50], label=\"LGBM\")\n",
    "    plt.plot(y[0:50]*0.5+0.25)\n",
    "\n",
    "    mse_gmb = mean_squared_error(y_test.ravel(), pred2_gmb)\n",
    "    mae_gmb = mean_absolute_error(y_test.ravel(), pred2_gmb)\n",
    "    r2_gmb = r2_score(y_test.ravel(), pred2_gmb)\n",
    "    print('mse: %.4f' % mse_gmb)\n",
    "    print('mae: %.4f' % mae_gmb)\n",
    "    print('r2: %.4f' % r2_gmb)\n",
    "\n",
    "    print(\"\\nXGBoost Regression:\")\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', max_depth = 15, learning_rate = 0.005, n_estimators = 150,  n_jobs = -1)\n",
    "    xg_reg.fit(x_train, y_train)\n",
    "    pred_xg = xg_reg.predict(x_test)\n",
    "\n",
    "    plt.plot(pred_xg[0:50], label=\"XGB\")\n",
    "    plt.legend()\n",
    "    plt.ylim(top=1, bottom=0)\n",
    "    mse_xg = mean_squared_error(y_test, pred_xg)\n",
    "    mae_xg = mean_absolute_error(y_test, pred_xg)\n",
    "    r2_xg = r2_score(y_test, pred_xg)\n",
    "    print('mse: %.4f' % mse_xg)\n",
    "    print('mae: %.4f' % mae_xg)\n",
    "    print('r2: %.4f' % r2_xg)\n",
    "\n",
    "        #增加特征多样性与样本多样性\n",
    "    clf2 = RFR(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=1412,n_jobs=8)\n",
    "    #特征多样性，稍微上调特征数量\n",
    "    clf3 = GBR(n_estimators= 100,max_features=16,random_state=1412) \n",
    "\n",
    "    #增加算法多样性，新增决策树与KNN\n",
    "    clf4 = DTR(max_depth=8,random_state=1412)\n",
    "    clf5 = KNNR(n_neighbors=10,n_jobs=8)\n",
    "    clf6 = GaussianNB()\n",
    "\n",
    "    #新增随机多样性，相同的算法更换随机数种子\n",
    "    clf7 = RFR(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=4869,n_jobs=8, min_impurity_decrease=0.0025)\n",
    "    clf8 = GBR(n_estimators= 100,max_features=16,random_state=4869)\n",
    "\n",
    "    final_estimator = RFR(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=3217,n_jobs=8, min_impurity_decrease=0.0025)\n",
    "\n",
    "    estimators = [(\"RandomForest\", clf2)\n",
    "                  , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5) \n",
    "                  , (\"RandomForest2\", clf7), (\"GBDT2\", clf8)]\n",
    "\n",
    "    print(\"\\nStacking:\")\n",
    "    stack = StackingRegressor(estimators, final_estimator)\n",
    "    stack.fit(x_train, y_train)\n",
    "    pred_stack = stack.predict(x_test)\n",
    "    mse_stack = mean_squared_error(y_test, pred_stack)\n",
    "    mae_stack = mean_absolute_error(y_test, pred_stack)\n",
    "    r2_stack = r2_score(y_test, pred_stack)\n",
    "    plt.plot(pred_stack[0:50], label=\"Stack\")\n",
    "    print('mse: %.4f' % mse_stack)\n",
    "    print('mae: %.4f' % mae_stack)\n",
    "    print('r2: %.4f' % r2_stack)\n",
    "    \n",
    "    print(precision_score(y_test, preprocessing.binarize(pd.DataFrame(pred_stack), threshold=0.5)))\n",
    "\n",
    "    return pred_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataset(x_train, y_train, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d7c0c2199f1ab9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
